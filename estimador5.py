
# app.py
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import json
import base64
from datetime import datetime
import io
import warnings
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import RFE
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy import stats

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential, save_model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Fuzzy Logic
import skfuzzy as fuzz
from skfuzzy import control as ctrl

# Statistical tests
from statsmodels.sandbox.stats.runs import mcnemar
from sklearn.metrics import matthews_corrcoef

# Report generation
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors
from reportlab.lib.units import inch
import matplotlib.backends.backend_pdf

# Set page configuration
st.set_page_config(
    page_title="Estimador de Precios de Viviendas",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 2rem;
        color: #1f77b4;
        border-bottom: 2px solid #1f77b4;
        padding-bottom: 0.5rem;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .highlight {
        background-color: #e6f2ff;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f0f8ff;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
</style>
""", unsafe_allow_html=True)

# Translations
translations = {
    "es": {
        "title": "Estimador de Precios de Viviendas",
        "upload_data": "Cargar Datos",
        "file_types": "Seleccione un archivo CSV o Excel",
        "eda": "An√°lisis Exploratorio de Datos",
        "preprocessing": "Preprocesamiento de Datos",
        "modeling": "Modelado",
        "evaluation": "Evaluaci√≥n de Modelos",
        "report": "Reporte Final",
        "target_var": "Seleccione la variable objetivo",
        "run_eda": "Ejecutar An√°lisis Exploratorio",
        "data_dimensions": "Dimensiones del Dataset",
        "num_rows": "N√∫mero de Filas",
        "num_cols": "N√∫mero de Columnas",
        "numeric_vars": "Variables Num√©ricas",
        "categorical_vars": "Variables Categ√≥ricas",
        "missing_values": "Valores Faltantes",
        "duplicates": "Valores Duplicados",
        "descriptive_stats": "Estad√≠sticas Descriptivas",
        "correlation_matrix": "Matriz de Correlaci√≥n",
        "outlier_detection": "Detecci√≥n de Outliers",
        "preprocess_data": "Preprocesar Datos",
        "train_models": "Entrenar Modelos",
        "evaluate_models": "Evaluar Modelos",
        "generate_report": "Generar Reporte",
        "results": "Resultados",
        "model_comparison": "Comparaci√≥n de Modelos",
        "best_model": "Mejor Modelo",
        "training_time": "Tiempo de Entrenamiento",
        "important_features": "Caracter√≠sticas Importantes",
        "mae": "Error Absoluto Medio (MAE)",
        "rmse": "Ra√≠z del Error Cuadr√°tico Medio (RMSE)",
        "r2": "Coeficiente de Determinaci√≥n (R¬≤)",
        "confusion_matrix": "Matriz de Confusi√≥n",
        "roc_curve": "Curva ROC",
        "auc": "Curva AUC",
        "mcc": "Coeficiente de Matthews (MCC)",
        "mcnemar_test": "Prueba de McNemar",
        "theil_coefficient": "Coeficiente de Theil",
        "diebold_mariano": "Prueba de Diebold-Mariano"
    },
    "en": {
        "title": "Housing Price Estimator",
        "upload_data": "Upload Data",
        "file_types": "Select a CSV or Excel file",
        "eda": "Exploratory Data Analysis",
        "preprocessing": "Data Preprocessing",
        "modeling": "Modeling",
        "evaluation": "Model Evaluation",
        "report": "Final Report",
        "target_var": "Select target variable",
        "run_eda": "Run Exploratory Analysis",
        "data_dimensions": "Dataset Dimensions",
        "num_rows": "Number of Rows",
        "num_cols": "Number of Columns",
        "numeric_vars": "Numeric Variables",
        "categorical_vars": "Categorical Variables",
        "missing_values": "Missing Values",
        "duplicates": "Duplicate Values",
        "descriptive_stats": "Descriptive Statistics",
        "correlation_matrix": "Correlation Matrix",
        "outlier_detection": "Outlier Detection",
        "preprocess_data": "Preprocess Data",
        "train_models": "Train Models",
        "evaluate_models": "Evaluate Models",
        "generate_report": "Generate Report",
        "results": "Results",
        "model_comparison": "Model Comparison",
        "best_model": "Best Model",
        "training_time": "Training Time",
        "important_features": "Important Features",
        "mae": "Mean Absolute Error (MAE)",
        "rmse": "Root Mean Squared Error (RMSE)",
        "r2": "Coefficient of Determination (R¬≤)",
        "confusion_matrix": "Confusion Matrix",
        "roc_curve": "ROC Curve",
        "auc": "AUC Curve",
        "mcc": "Matthews Correlation Coefficient (MCC)",
        "mcnemar_test": "McNemar Test",
        "theil_coefficient": "Theil Coefficient",
        "diebold_mariano": "Diebold-Mariano Test"
    },
    "fr": {
        "title": "Estimateur de Prix Immobiliers",
        "upload_data": "Charger les Donn√©es",
        "file_types": "S√©lectionnez un fichier CSV ou Excel",
        "eda": "Analyse Exploratoire des Donn√©es",
        "preprocessing": "Pr√©traitement des Donn√©es",
        "modeling": "Mod√©lisation",
        "evaluation": "√âvaluation des Mod√®les",
        "report": "Rapport Final",
        "target_var": "S√©lectionnez la variable cible",
        "run_eda": "Ex√©cuter l'Analyse Exploratoire",
        "data_dimensions": "Dimensions du Jeu de Donn√©es",
        "num_rows": "Nombre de Lignes",
        "num_cols": "Nombre de Colonnes",
        "numeric_vars": "Variables Num√©riques",
        "categorical_vars": "Variables Cat√©gorielles",
        "missing_values": "Valeurs Manquantes",
        "duplicates": "Valeurs Dupliqu√©es",
        "descriptive_stats": "Statistiques Descriptives",
        "correlation_matrix": "Matrice de Corr√©lation",
        "outlier_detection": "D√©tection des Valeurs Aberrantes",
        "preprocess_data": "Pr√©traiter les Donn√©es",
        "train_models": "Entra√Æner les Mod√®les",
        "evaluate_models": "√âvaluer les Mod√®les",
        "generate_report": "G√©n√©rer le Rapport",
        "results": "R√©sultats",
        "model_comparison": "Comparaison des Mod√®les",
        "best_model": "Meilleur Mod√®le",
        "training_time": "Temps d'Entra√Ænement",
        "important_features": "Caract√©ristiques Importantes",
        "mae": "Erreur Absolue Moyenne (MAE)",
        "rmse": "Racine de l'Erreur Quadratique Moyenne (RMSE)",
        "r2": "Coefficient de D√©termination (R¬≤)",
        "confusion_matrix": "Matrice de Confusion",
        "roc_curve": "Courbe ROC",
        "auc": "Courbe AUC",
        "mcc": "Coefficient de Corr√©lation de Matthews (MCC)",
        "mcnemar_test": "Test de McNemar",
        "theil_coefficient": "Coefficient de Theil",
        "diebold_mariano": "Test de Diebold-Mariano"
    }
}

# Initialize session state
if 'df' not in st.session_state:
    st.session_state.df = None
if 'target_var' not in st.session_state:
    st.session_state.target_var = None
if 'language' not in st.session_state:
    st.session_state.language = 'es'
if 'eda_done' not in st.session_state:
    st.session_state.eda_done = False
if 'preprocessing_done' not in st.session_state:
    st.session_state.preprocessing_done = False
if 'models_trained' not in st.session_state:
    st.session_state.models_trained = False
if 'models' not in st.session_state:
    st.session_state.models = {}
if 'X_train' not in st.session_state:
    st.session_state.X_train = None
if 'X_val' not in st.session_state:
    st.session_state.X_val = None
if 'X_test' not in st.session_state:
    st.session_state.X_test = None
if 'y_train' not in st.session_state:
    st.session_state.y_train = None
if 'y_val' not in st.session_state:
    st.session_state.y_val = None
if 'y_test' not in st.session_state:
    st.session_state.y_test = None

# Helper functions
def t(key):
    return translations[st.session_state.language][key]

def load_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        return df
    except Exception as e:
        st.error(f"Error loading file: {str(e)}")
        return None

def perform_eda(df, target_var):
    st.subheader(t('data_dimensions'))
    col1, col2 = st.columns(2)
    col1.metric(t('num_rows'), df.shape[0])
    col2.metric(t('num_cols'), df.shape[1])
    
    st.subheader(t('numeric_vars'))
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    st.write(numeric_cols)
    
    st.subheader(t('categorical_vars'))
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    st.write(categorical_cols)
    
    st.subheader(t('missing_values'))
    missing_data = df.isnull().sum()
    missing_df = pd.DataFrame({
        'Column': missing_data.index,
        'Missing Values': missing_data.values,
        'Percentage': (missing_data.values / len(df)) * 100
    })
    st.dataframe(missing_df[missing_df['Missing Values'] > 0])
    
    st.subheader(t('duplicates'))
    duplicates = df.duplicated().sum()
    st.metric("Duplicated Rows", duplicates)
    
    st.subheader(t('descriptive_stats'))
    st.dataframe(df.describe())
    
    # Visualizations
    st.subheader("Distributions")
    
    # Numeric variables distributions
    if numeric_cols:
        num_cols_to_show = min(5, len(numeric_cols))
        fig, axes = plt.subplots(1, num_cols_to_show, figsize=(20, 4))
        if num_cols_to_show == 1:
            axes = [axes]
        
        for i, col in enumerate(numeric_cols[:num_cols_to_show]):
            axes[i].hist(df[col].dropna(), bins=30, alpha=0.7, color='blue')
            axes[i].set_title(f'Distribution of {col}')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('Frequency')
        
        plt.tight_layout()
        st.pyplot(fig)
    
    # Correlation matrix
    st.subheader(t('correlation_matrix'))
    if len(numeric_cols) > 1:
        corr_matrix = df[numeric_cols].corr()
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
        st.pyplot(fig)
        
        # Top correlations with target
        if target_var in numeric_cols:
            st.write(f"Top correlations with {target_var}:")
            target_correlations = corr_matrix[target_var].abs().sort_values(ascending=False)
            st.write(target_correlations[1:6])  # Top 5 excluding itself
    
    # Outlier detection
    st.subheader(t('outlier_detection'))
    if numeric_cols:
        num_cols_to_show = min(3, len(numeric_cols))
        fig, axes = plt.subplots(1, num_cols_to_show, figsize=(15, 5))
        if num_cols_to_show == 1:
            axes = [axes]
        
        for i, col in enumerate(numeric_cols[:num_cols_to_show]):
            axes[i].boxplot(df[col].dropna())
            axes[i].set_title(f'Boxplot of {col}')
        
        plt.tight_layout()
        st.pyplot(fig)
    
    return True

def preprocess_data(df, target_var, options):
    df_processed = df.copy()
    
    # Handle missing values
    if options['handle_missing'] == 'Drop rows':
        df_processed = df_processed.dropna()
    elif options['handle_missing'] == 'Mean imputation':
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
        imputer = SimpleImputer(strategy='mean')
        df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])
    elif options['handle_missing'] == 'KNN imputation':
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
        imputer = KNNImputer(n_neighbors=5)
        df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])
    
    # Handle duplicates
    if options['handle_duplicates']:
        df_processed = df_processed.drop_duplicates()
    
    # Handle outliers with Z-score
    if options['handle_outliers']:
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col != target_var:  # Don't remove outliers from target variable
                z_scores = np.abs(stats.zscore(df_processed[col].dropna()))
                df_processed = df_processed[(z_scores < 3) | (df_processed[col].isna())]
    
    # Encode categorical variables
    categorical_cols = df_processed.select_dtypes(include=['object']).columns
    if options['encoding'] == 'One-Hot Encoding':
        df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)
    elif options['encoding'] == 'Label Encoding':
        le = LabelEncoder()
        for col in categorical_cols:
            df_processed[col] = le.fit_transform(df_processed[col].astype(str))
    
    # Calculate VIF and handle multicollinearity
    if options['handle_multicollinearity']:
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
        vif_data = pd.DataFrame()
        vif_data["Feature"] = numeric_cols
        vif_data["VIF"] = [variance_inflation_factor(df_processed[numeric_cols].values, i) 
                          for i in range(len(numeric_cols))]
        
        # Remove features with high VIF
        high_vif_features = vif_data[vif_data["VIF"] > 10]["Feature"].tolist()
        if target_var in high_vif_features:
            high_vif_features.remove(target_var)
        df_processed = df_processed.drop(columns=high_vif_features)
    
    # Scale features
    if options['scaling']:
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
        if target_var in numeric_cols:
            numeric_cols = numeric_cols.drop(target_var)
        scaler = StandardScaler()
        df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])
    
    return df_processed

def train_linear_regression(X_train, y_train):
    start_time = time.time()
    model = LinearRegression()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    # Get feature importance
    importance = np.abs(model.coef_)
    feature_importance = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importance
    }).sort_values('Importance', ascending=False)
    
    return model, training_time, feature_importance

def train_neural_network(X_train, y_train, X_val, y_val):
    start_time = time.time()
    
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1)
    ])
    
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
    
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=32,
        callbacks=[early_stopping],
        verbose=0
    )
    
    training_time = time.time() - start_time
    
    # Get feature importance using permutation importance (simplified)
    baseline_mae = mean_absolute_error(y_train, model.predict(X_train, verbose=0).flatten())
    feature_importance = []
    
    for i in range(X_train.shape[1]):
        X_temp = X_train.copy()
        np.random.shuffle(X_temp[:, i])
        mae_score = mean_absolute_error(y_train, model.predict(X_temp, verbose=0).flatten())
        importance = mae_score - baseline_mae
        feature_importance.append((X_train.columns[i], importance))
    
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    feature_importance_df = pd.DataFrame(feature_importance, columns=['Feature', 'Importance'])
    
    return model, training_time, feature_importance_df, history

def train_mlp(X_train, y_train, X_val, y_val):
    start_time = time.time()
    
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
    
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=32,
        callbacks=[early_stopping],
        verbose=0
    )
    
    training_time = time.time() - start_time
    
    # Get feature importance
    baseline_mae = mean_absolute_error(y_train, model.predict(X_train, verbose=0).flatten())
    feature_importance = []
    
    for i in range(X_train.shape[1]):
        X_temp = X_train.copy()
        np.random.shuffle(X_temp[:, i])
        mae_score = mean_absolute_error(y_train, model.predict(X_temp, verbose=0).flatten())
        importance = mae_score - baseline_mae
        feature_importance.append((X_train.columns[i], importance))
    
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    feature_importance_df = pd.DataFrame(feature_importance, columns=['Feature', 'Importance'])
    
    return model, training_time, feature_importance_df, history

def train_fuzzy_neural_network(X_train, y_train):
    # This is a simplified implementation of a fuzzy neural network
    # In practice, this would be more complex
    start_time = time.time()
    
    # Create fuzzy variables
    fuzzy_vars = []
    for i, col in enumerate(X_train.columns):
        var = ctrl.Antecedent(np.arange(X_train[col].min(), X_train[col].max(), 0.1), col)
        var.automf(3)  # Create 3 membership functions
        fuzzy_vars.append(var)
    
    # Create output variable
    output = ctrl.Consequent(np.arange(y_train.min(), y_train.max(), 0.1), 'output', 'centroid')
    output.automf(3)
    
    # Create rules (simplified)
    rules = []
    for i in range(min(10, len(X_train))):  # Limit rules for simplicity
        rule_conditions = []
        for j, col in enumerate(X_train.columns):
            rule_conditions.append(fuzzy_vars[j]['average'])
        
        rule = ctrl.Rule(rule_conditions, output['average'])
        rules.append(rule)
    
    # Create control system
    system = ctrl.ControlSystem(rules)
    sim = ctrl.ControlSystemSimulation(system)
    
    # Train the system (simplified)
    training_time = time.time() - start_time
    
    # For the purpose of this demo, we'll return a simple linear model
    # In a real implementation, this would be much more complex
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # Feature importance (using linear model as proxy)
    importance = np.abs(model.coef_)
    feature_importance = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importance
    }).sort_values('Importance', ascending=False)
    
    return model, training_time, feature_importance, {}

def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    
    if hasattr(model, 'predict'):
        # Scikit-learn model
        y_pred = model.predict(X_test)
    else:
        # Keras model
        y_pred = model.predict(X_test, verbose=0).flatten()
    
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    
    return {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2
    }

def generate_pdf_report(df, eda_results, models, evaluation_results, best_model):
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []
    
    # Title
    title = Paragraph("Housing Price Estimation Report", styles['Title'])
    story.append(title)
    story.append(Spacer(1, 12))
    
    # Dataset information
    story.append(Paragraph("Dataset Information", styles['Heading2']))
    data = [["Number of rows", len(df)], ["Number of columns", len(df.columns)]]
    table = Table(data)
    story.append(table)
    story.append(Spacer(1, 12))
    
    # EDA results
    story.append(Paragraph("Exploratory Data Analysis", styles['Heading2']))
    # Add EDA results here...
    
    # Model comparison
    story.append(Paragraph("Model Comparison", styles['Heading2']))
    model_data = [["Model", "MAE", "RMSE", "R2"]]
    for model_name, metrics in evaluation_results.items():
        model_data.append([model_name, f"{metrics['MAE']:.4f}", f"{metrics['RMSE']:.4f}", f"{metrics['R2']:.4f}"])
    
    model_table = Table(model_data)
    story.append(model_table)
    story.append(Spacer(1, 12))
    
    # Best model
    story.append(Paragraph(f"Best Model: {best_model}", styles['Heading2']))
    best_metrics = evaluation_results[best_model]
    best_data = [["Metric", "Value"], 
                 ["MAE", f"{best_metrics['MAE']:.4f}"],
                 ["RMSE", f"{best_metrics['RMSE']:.4f}"],
                 ["R2", f"{best_metrics['R2']:.4f}"]]
    best_table = Table(best_data)
    story.append(best_table)
    
    doc.build(story)
    buffer.seek(0)
    return buffer

# Sidebar
with st.sidebar:
    st.title(t('title'))
    
    # Language selection
    language = st.radio("Select Language / S√©lectionnez la langue / Seleccionar idioma", 
                       options=['en', 'fr', 'es'],
                       index=['en', 'fr', 'es'].index(st.session_state.language))
    st.session_state.language = language
    
    # Data upload
    st.header(t('upload_data'))
    uploaded_file = st.file_uploader(t('file_types'), type=['csv', 'xlsx', 'xls'])
    
    if uploaded_file is not None:
        if st.session_state.df is None:
            st.session_state.df = load_data(uploaded_file)
        
        if st.session_state.df is not None:
            df = st.session_state.df
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if numeric_cols:
                st.session_state.target_var = st.selectbox(t('target_var'), numeric_cols)
                
                # EDA button
                if st.button(t('run_eda')):
                    with st.spinner('Performing EDA...'):
                        st.session_state.eda_done = perform_eda(df, st.session_state.target_var)
                
                # Preprocessing options
                st.header(t('preprocessing'))
                preprocessing_options = {
                    'handle_missing': st.selectbox('Handle missing values', 
                                                  ['Drop rows', 'Mean imputation', 'KNN imputation']),
                    'handle_duplicates': st.checkbox('Remove duplicates', value=True),
                    'handle_outliers': st.checkbox('Handle outliers (Z-score)', value=True),
                    'encoding': st.selectbox('Encode categorical variables',
                                           ['One-Hot Encoding', 'Label Encoding']),
                    'handle_multicollinearity': st.checkbox('Handle multicollinearity (VIF)', value=True),
                    'scaling': st.checkbox('Scale features', value=True)
                }
                
                if st.button(t('preprocess_data')):
                    with st.spinner('Preprocessing data...'):
                        df_processed = preprocess_data(df, st.session_state.target_var, preprocessing_options)
                        
                        # Split data
                        X = df_processed.drop(columns=[st.session_state.target_var])
                        y = df_processed[st.session_state.target_var]
                        
                        X_temp, X_test, y_temp, y_test = train_test_split(
                            X, y, test_size=0.15, random_state=42
                        )
                        X_train, X_val, y_train, y_val = train_test_split(
                            X_temp, y_temp, test_size=0.1765, random_state=42  # 0.15/0.85 ‚âà 0.1765
                        )
                        
                        st.session_state.X_train = X_train
                        st.session_state.X_val = X_val
                        st.session_state.X_test = X_test
                        st.session_state.y_train = y_train
                        st.session_state.y_val = y_val
                        st.session_state.y_test = y_test
                        
                        st.session_state.preprocessing_done = True
                        st.success('Data preprocessing completed!')
                
                # Model training
                if st.session_state.preprocessing_done:
                    st.header(t('modeling'))
                    if st.button(t('train_models')):
